{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b67cac-14d2-4901-b67b-d606ae807ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T13:33:52.916991Z",
     "iopub.status.busy": "2021-11-01T13:33:52.916575Z",
     "iopub.status.idle": "2021-11-01T13:33:52.923920Z",
     "shell.execute_reply": "2021-11-01T13:33:52.923173Z",
     "shell.execute_reply.started": "2021-11-01T13:33:52.916954Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# s3 params\n",
    "with open('s3_params.txt', 'r') as f:\n",
    "    s3_params = json.loads(f.read())\n",
    "s3_key = s3_params['access-key']\n",
    "s3_secret = s3_params['secret-key']\n",
    "s3_url = s3_params['endpoint_url']\n",
    "s3_binance_cred_path = s3_params['cred_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9d7988-1913-428e-99c6-94fa5a0a2595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T13:33:55.504850Z",
     "iopub.status.busy": "2021-11-01T13:33:55.503929Z",
     "iopub.status.idle": "2021-11-01T13:33:55.512524Z",
     "shell.execute_reply": "2021-11-01T13:33:55.510386Z",
     "shell.execute_reply.started": "2021-11-01T13:33:55.504779Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read schema from file\n",
    "with open('schema.txt', 'r') as f:\n",
    "    schema_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99ee2a6-9827-4bec-ab3d-95ef0331e3ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T13:33:57.612783Z",
     "iopub.status.busy": "2021-11-01T13:33:57.612160Z",
     "iopub.status.idle": "2021-11-01T13:33:57.617843Z",
     "shell.execute_reply": "2021-11-01T13:33:57.616530Z",
     "shell.execute_reply.started": "2021-11-01T13:33:57.612743Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from s3fs.core import S3FileSystem\n",
    "# s3fs client\n",
    "s3 = S3FileSystem(\n",
    "     anon=False,\n",
    "     key=s3_key,\n",
    "     secret=s3_secret,\n",
    "     use_ssl=False,\n",
    "     client_kwargs={'endpoint_url': s3_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a869444-19ab-49d5-8d29-29327c582a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T13:34:01.802499Z",
     "iopub.status.busy": "2021-11-01T13:34:01.801558Z",
     "iopub.status.idle": "2021-11-01T13:34:01.810147Z",
     "shell.execute_reply": "2021-11-01T13:34:01.808784Z",
     "shell.execute_reply.started": "2021-11-01T13:34:01.802459Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "config = {\n",
    "    \"spark.kubernetes.namespace\": \"spark\",\n",
    "    \"spark.kubernetes.container.image\": \"localhost:32000/spark:3.1.2-hadoop-3.2.0-aws\",\n",
    "    \"spark.executor.instances\": \"1\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.blockManager.port\": \"7777\",\n",
    "    \"spark.driver.port\": \"2222\",\n",
    "    \"spark.driver.host\": \"jupyter.spark.svc.cluster.local\",\n",
    "    \"spark.driver.bindAddress\": \"0.0.0.0\",\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": s3_url,\n",
    "    \"spark.hadoop.fs.s3a.access.key\": s3_key,\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": s3_secret,\n",
    "    \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.com.amazonaws.services.s3.enableV4\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    \"spark.hadoop.fs.s3a.committer.name\": \"directory\",\n",
    "    \"spark.sql.sources.commitProtocolClass\": \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\",\n",
    "    \"spark.sql.parquet.output.committer.class\": \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\"\n",
    "}\n",
    "\n",
    "def get_spark_session(app_name: str, conf: SparkConf):\n",
    "    conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local\")\n",
    "    for key, value in config.items():\n",
    "        conf.set(key, value)    \n",
    "    return SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bdbd96c-7548-4531-9c05-7f0eeb1cf4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T13:34:05.493637Z",
     "iopub.status.busy": "2021-11-01T13:34:05.493184Z",
     "iopub.status.idle": "2021-11-01T13:34:54.089299Z",
     "shell.execute_reply": "2021-11-01T13:34:54.088499Z",
     "shell.execute_reply.started": "2021-11-01T13:34:05.493592Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/11/01 16:34:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session(\"aws_localstack\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae6595f-1876-4c63-8fbb-d07625c311f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-01T13:36:28.376361Z",
     "iopub.status.busy": "2021-11-01T13:36:28.375233Z",
     "iopub.status.idle": "2021-11-01T13:36:28.387078Z",
     "shell.execute_reply": "2021-11-01T13:36:28.385838Z",
     "shell.execute_reply.started": "2021-11-01T13:36:28.376264Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a1a3d-c059-473a-8768-b32738d9bf1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = spark.read.csv('s3a://data/test/BTCUSDT_MinuteBars.csv', header=True)\n",
    "    df.printSchema()\n",
    "    print(df.count())\n",
    "except Exception as exp:\n",
    "    print(exp)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd8e42d5-4b40-48da-82c3-691d22277cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-15T21:17:32.170066Z",
     "iopub.status.busy": "2021-10-15T21:17:32.168536Z",
     "iopub.status.idle": "2021-10-15T21:17:38.109530Z",
     "shell.execute_reply": "2021-10-15T21:17:38.108690Z",
     "shell.execute_reply.started": "2021-10-15T21:17:32.169932Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found duplicate column(s) in the data schema: `e`, `k_l`, `k_q`, `k_t`, `k_v`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 21:17:38 WARN DataSource: Found duplicate column(s) in the data schema and the partition schema: `e`, `k_l`, `k_q`, `k_t`, `k_v`\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session(\"localstack\", conf)\n",
    "\n",
    "try:\n",
    "    df = spark.read.parquet('s3a://data/test/test.parquet/')\n",
    "    df.printSchema()\n",
    "    print(df.count())\n",
    "except Exception as exp:\n",
    "    print(exp)\n",
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab91bbb5-61c6-402f-b93e-acf82782809a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T08:57:10.030191Z",
     "iopub.status.busy": "2021-10-12T08:57:10.029925Z",
     "iopub.status.idle": "2021-10-12T08:57:10.075196Z",
     "shell.execute_reply": "2021-10-12T08:57:10.074255Z",
     "shell.execute_reply.started": "2021-10-12T08:57:10.030167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from s3fs.core import S3FileSystem\n",
    "from pyarrow import Table, parquet as pq\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "def to_df(data: List[Dict[str, Any]]) -> DataFrame:                                                                                                         \n",
    "    df = DataFrame()\n",
    "    for item in data:\n",
    "        indexes = []\n",
    "        values = []\n",
    "        for k, v in item.items():\n",
    "            indexes.append(k)\n",
    "            values.append(v)\n",
    "        df = df.append(Series(values, index=indexes), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# s3fs client\n",
    "fs = S3FileSystem(\n",
    "     anon=False,\n",
    "     key=s3_key,\n",
    "     secret=s3_secret,\n",
    "     use_ssl=False,\n",
    "     client_kwargs={'endpoint_url': s3_url})\n",
    "\n",
    "path_to_s3_object = \"s3://data/test/test.parquet\"\n",
    "\n",
    "data = [\n",
    "  {\n",
    "    \"hoge\": 1,\n",
    "    \"foo\": \"blah\",\n",
    "  },\n",
    "  {\n",
    "    \"boo\": \"test\",\n",
    "    \"bar\": 123,\n",
    "  },\n",
    "]\n",
    "df = to_df(data)\n",
    "pq.write_to_dataset(\n",
    "    Table.from_pandas(df),\n",
    "    path_to_s3_object,\n",
    "    filesystem=fs,\n",
    "    use_dictionary=True,\n",
    "    #compression=\"snappy\",\n",
    "    version=\"2.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0404cf-3471-41b7-be50-cc964456e5b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T08:59:48.593800Z",
     "iopub.status.busy": "2021-10-12T08:59:48.592801Z",
     "iopub.status.idle": "2021-10-12T08:59:49.060619Z",
     "shell.execute_reply": "2021-10-12T08:59:49.059188Z",
     "shell.execute_reply.started": "2021-10-12T08:59:48.593712Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from confluent_kafka import DeserializingConsumer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer\n",
    "from confluent_kafka.serialization import StringDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c52e7-6cd3-48c1-81f5-8d2e80136c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    topic = args.topic\n",
    "\n",
    "    schema_registry_conf = {'url': args.schema_registry}\n",
    "\n",
    "    schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "    avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "    \n",
    "    string_deserializer = StringDeserializer('utf_8')\n",
    "\n",
    "    consumer_conf = {'bootstrap.servers': args.bootstrap_servers,\n",
    "                     'key.deserializer': string_deserializer,\n",
    "                     'value.deserializer': avro_deserializer,\n",
    "                     'group.id': args.group,\n",
    "                     'auto.offset.reset': \"earliest\"}\n",
    "\n",
    "    consumer = DeserializingConsumer(consumer_conf)\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
    "            msg = consumer.poll(1.0)\n",
    "            print(msg.value())\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    consumer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"AvroDeserializingProducer\")\n",
    "    parser.add_argument('-b', dest=\"bootstrap_servers\",\n",
    "                        default=\"kafka.kafka.svc.cluster.local:9092\", help=\"Bootstrap servers\")\n",
    "    parser.add_argument('-r', dest=\"schema_registry\",\n",
    "                        default=\"http://schema-registry.kafka.svc.cluster.local:8085\", help=\"Schema registry url\")\n",
    "    parser.add_argument('-t', dest=\"topic\", default=\"test\", help=\"Topic\")\n",
    "    parser.add_argument('-s', dest=\"symbol\", default=\"BTCUSDT\", help=\"Symbol\")\n",
    "    parser.add_argument('-g', dest=\"group\", default=\"test\", help=\"Consumer group\")\n",
    "    main(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1118248d-b411-4ac5-9d6e-5d92725408f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T17:45:55.203075Z",
     "iopub.status.busy": "2021-10-30T17:45:55.202811Z",
     "iopub.status.idle": "2021-10-30T17:46:03.240626Z",
     "shell.execute_reply": "2021-10-30T17:46:03.239351Z",
     "shell.execute_reply.started": "2021-10-30T17:45:55.203051Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n",
      "none\n",
      "none\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%5|1635615963.227|REQTMOUT|rdkafka#consumer-2| [thrd:GroupCoordinator]: GroupCoordinator/0: Timed out LeaveGroupRequest in flight (after 5003ms, timeout #0): possibly held back by preceeding blocking JoinGroupRequest with timeout in 295009ms\n",
      "%4|1635615963.227|REQTMOUT|rdkafka#consumer-2| [thrd:GroupCoordinator]: GroupCoordinator/0: Timed out 1 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n",
      "%3|1635615963.227|FAIL|rdkafka#consumer-2| [thrd:GroupCoordinator]: GroupCoordinator: kafka-0.kafka-headless.kafka.svc.cluster.local:9092: 1 request(s) timed out: disconnect (after 7993ms in state UP)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf = to_df(data)\\npq.write_to_dataset(\\n    Table.from_pandas(df),\\n    path_to_s3_object,\\n    filesystem=fs,\\n    use_dictionary=True,\\n    #compression=\"snappy\",\\n    version=\"2.0\",\\n)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "from s3fs.core import S3FileSystem\n",
    "from pyarrow import Table, parquet as pq\n",
    "from pandas import DataFrame, Series\n",
    "import argparse\n",
    "from confluent_kafka import DeserializingConsumer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer\n",
    "from confluent_kafka.serialization import StringDeserializer\n",
    "import json\n",
    "\n",
    "# s3 params\n",
    "with open('s3_params.txt', 'r') as f:\n",
    "    s3_params = json.loads(f.read())\n",
    "s3_key = s3_params['access-key']\n",
    "s3_secret = s3_params['secret-key']\n",
    "s3_url = s3_params['endpoint_url']\n",
    "s3_binance_cred_path = s3_params['cred_path']\n",
    "\n",
    "# Read schema from file\n",
    "with open('schema.txt', 'r') as f:\n",
    "    schema_str = f.read()\n",
    "\n",
    "def to_df(data: List[Dict[str, Any]]) -> DataFrame:                                                                                                         \n",
    "    df = DataFrame()\n",
    "    #for item in data:\n",
    "    indexes = []\n",
    "    values = []\n",
    "    for k, v in data.items():\n",
    "        indexes.append(k)\n",
    "        values.append(v)\n",
    "    df = df.append(Series(values, index=indexes), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# s3fs client\n",
    "fs = S3FileSystem(\n",
    "     anon=False,\n",
    "     key=s3_key,\n",
    "     secret=s3_secret,\n",
    "     use_ssl=False,\n",
    "     client_kwargs={'endpoint_url': s3_url})\n",
    "\n",
    "path_to_s3_object = \"s3://data/test/test.parquet\"\n",
    "\n",
    "topic = \"test\"\n",
    "\n",
    "schema_registry_conf = {\"url\": \"http://schema-registry.kafka.svc.cluster.local:8085\"}\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "\n",
    "string_deserializer = StringDeserializer('utf_8')\n",
    "\n",
    "consumer_conf = {'bootstrap.servers': \"kafka.kafka.svc.cluster.local:9092\",\n",
    "                 'key.deserializer': string_deserializer,\n",
    "                 'value.deserializer': avro_deserializer,\n",
    "                 'group.id': \"test\",\n",
    "                 #'auto.offset.reset': \"earliest\"\n",
    "                }\n",
    "\n",
    "consumer = DeserializingConsumer(consumer_conf)\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < 3:\n",
    "    i = i+1\n",
    "    try:\n",
    "        # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            print(\"none\")\n",
    "            continue\n",
    "            \n",
    "        if msg is not None:\n",
    "            print(hash(str(msg.value())))\n",
    "            data = flatten_json(msg.value()) \n",
    "            print(type(data), data)\n",
    "            \n",
    "            df = to_df(data)\n",
    "            print(type(df), df)\n",
    "            pq.write_to_dataset(\n",
    "                Table.from_pandas(df),\n",
    "                path_to_s3_object,\n",
    "                filesystem=fs,\n",
    "                use_dictionary=True,\n",
    "                #compression=\"snappy\",\n",
    "                version=\"2.0\",\n",
    "                )\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "consumer.close()\n",
    "\n",
    "\"\"\"\n",
    "df = to_df(data)\n",
    "pq.write_to_dataset(\n",
    "    Table.from_pandas(df),\n",
    "    path_to_s3_object,\n",
    "    filesystem=fs,\n",
    "    use_dictionary=True,\n",
    "    #compression=\"snappy\",\n",
    "    version=\"2.0\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29b0bfd-34e0-4eff-9a23-21dfc2d587ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T17:45:49.014595Z",
     "iopub.status.busy": "2021-10-30T17:45:49.014018Z",
     "iopub.status.idle": "2021-10-30T17:45:49.023181Z",
     "shell.execute_reply": "2021-10-30T17:45:49.022517Z",
     "shell.execute_reply.started": "2021-10-30T17:45:49.014550Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_json(nested_json):\n",
    "    \"\"\"\n",
    "\n",
    "Flatten json object with nested keys into a single level.\n",
    "        Args:\n",
    "            nested_json: A nested json object.\n",
    "        Returns:\n",
    "            The flattened json object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6a94908-b55b-4373-8fe4-6a5c9d8d2274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T07:02:02.811463Z",
     "iopub.status.busy": "2021-10-14T07:02:02.811134Z",
     "iopub.status.idle": "2021-10-14T07:02:02.822929Z",
     "shell.execute_reply": "2021-10-14T07:02:02.821326Z",
     "shell.execute_reply.started": "2021-10-14T07:02:02.811435Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "crumbs = False\n",
    "def flatten(dictionary, parent_key=False, separator='.'):\n",
    "    \"\"\"\n",
    "    Turn a nested dictionary into a flattened dictionary\n",
    "    :param dictionary: The dictionary to flatten\n",
    "    :param parent_key: The string to prepend to dictionary's keys\n",
    "    :param separator: The string used to separate flattened keys\n",
    "    :return: A flattened dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    items = []\n",
    "    for key, value in dictionary.items():\n",
    "        if crumbs: print('checking:',key)\n",
    "        new_key = str(parent_key) + separator + key if parent_key else key\n",
    "        if isinstance(value, collections.MutableMapping):\n",
    "            if crumbs: print(new_key,': dict found')\n",
    "            if not value.items():\n",
    "                if crumbs: print('Adding key-value pair:',new_key,None)\n",
    "                items.append((new_key,None))\n",
    "            else:\n",
    "                items.extend(flatten(value, new_key, separator).items())\n",
    "        elif isinstance(value, list):\n",
    "            if crumbs: print(new_key,': list found')\n",
    "            if len(value):\n",
    "                for k, v in enumerate(value):\n",
    "                    items.extend(flatten({str(k): v}, new_key).items())\n",
    "            else:\n",
    "                if crumbs: print('Adding key-value pair:',new_key,None)\n",
    "                items.append((new_key,None))\n",
    "        else:\n",
    "            if crumbs: print('Adding key-value pair:',new_key,value)\n",
    "            items.append((new_key, value))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e1ec6-ce19-40de-b43f-517085d3989c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
