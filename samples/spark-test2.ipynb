{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b67cac-14d2-4901-b67b-d606ae807ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T08:08:09.713350Z",
     "iopub.status.busy": "2021-10-12T08:08:09.712292Z",
     "iopub.status.idle": "2021-10-12T08:08:09.720054Z",
     "shell.execute_reply": "2021-10-12T08:08:09.719112Z",
     "shell.execute_reply.started": "2021-10-12T08:08:09.713296Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# s3 params\n",
    "with open('s3_params.txt', 'r') as f:\n",
    "    s3_params = json.loads(f.read())\n",
    "s3_key = s3_params['access-key']\n",
    "s3_secret = s3_params['secret-key']\n",
    "s3_url = s3_params['endpoint_url']\n",
    "s3_binance_cred_path = s3_params['cred_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e4b8bcd-af22-417f-9c18-7f316ddd73d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-17T21:33:06.627577Z",
     "iopub.status.busy": "2021-10-17T21:33:06.626910Z",
     "iopub.status.idle": "2021-10-17T21:33:06.655443Z",
     "shell.execute_reply": "2021-10-17T21:33:06.653898Z",
     "shell.execute_reply.started": "2021-10-17T21:33:06.627542Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9d7988-1913-428e-99c6-94fa5a0a2595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T11:18:53.460256Z",
     "iopub.status.busy": "2021-10-12T11:18:53.459895Z",
     "iopub.status.idle": "2021-10-12T11:18:53.466516Z",
     "shell.execute_reply": "2021-10-12T11:18:53.465373Z",
     "shell.execute_reply.started": "2021-10-12T11:18:53.460224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read schema from file\n",
    "with open('schema.txt', 'r') as f:\n",
    "    schema_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99ee2a6-9827-4bec-ab3d-95ef0331e3ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T08:44:33.756811Z",
     "iopub.status.busy": "2021-10-12T08:44:33.756418Z",
     "iopub.status.idle": "2021-10-12T08:44:33.761562Z",
     "shell.execute_reply": "2021-10-12T08:44:33.760886Z",
     "shell.execute_reply.started": "2021-10-12T08:44:33.756781Z"
    }
   },
   "outputs": [],
   "source": [
    "from s3fs.core import S3FileSystem\n",
    "# s3fs client\n",
    "s3 = S3FileSystem(\n",
    "     anon=False,\n",
    "     key=s3_key,\n",
    "     secret=s3_secret,\n",
    "     use_ssl=False,\n",
    "     client_kwargs={'endpoint_url': s3_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a869444-19ab-49d5-8d29-29327c582a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T08:52:40.758423Z",
     "iopub.status.busy": "2021-10-12T08:52:40.757995Z",
     "iopub.status.idle": "2021-10-12T08:52:40.766661Z",
     "shell.execute_reply": "2021-10-12T08:52:40.766127Z",
     "shell.execute_reply.started": "2021-10-12T08:52:40.758386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "config = {\n",
    "    \"spark.kubernetes.namespace\": \"spark\",\n",
    "    \"spark.kubernetes.container.image\": \"localhost:32000/spark:3.1.2-hadoop-3.2.0-aws\",\n",
    "    \"spark.executor.instances\": \"1\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.blockManager.port\": \"7777\",\n",
    "    \"spark.driver.port\": \"2222\",\n",
    "    \"spark.driver.host\": \"jupyter.spark.svc.cluster.local\",\n",
    "    \"spark.driver.bindAddress\": \"0.0.0.0\",\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": s3_url,\n",
    "    \"spark.hadoop.fs.s3a.access.key\": s3_key,\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": s3_secret,\n",
    "    \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.com.amazonaws.services.s3.enableV4\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    \"spark.hadoop.fs.s3a.committer.name\": \"directory\",\n",
    "    \"spark.sql.sources.commitProtocolClass\": \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\",\n",
    "    \"spark.sql.parquet.output.committer.class\": \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\"\n",
    "}\n",
    "\n",
    "def get_spark_session(app_name: str, conf: SparkConf):\n",
    "    conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local\")\n",
    "    for key, value in config.items():\n",
    "        conf.set(key, value)    \n",
    "    return SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bdbd96c-7548-4531-9c05-7f0eeb1cf4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T08:52:42.358051Z",
     "iopub.status.busy": "2021-10-12T08:52:42.357509Z",
     "iopub.status.idle": "2021-10-12T08:53:40.875229Z",
     "shell.execute_reply": "2021-10-12T08:53:40.874481Z",
     "shell.execute_reply.started": "2021-10-12T08:52:42.358008Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/12 08:53:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session(\"aws_localstack\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a1a3d-c059-473a-8768-b32738d9bf1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = spark.read.csv('s3a://data/test/BTCUSDT_MinuteBars.csv', header=True)\n",
    "    df.printSchema()\n",
    "    print(df.count())\n",
    "except Exception as exp:\n",
    "    print(exp)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8e42d5-4b40-48da-82c3-691d22277cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T08:53:51.182412Z",
     "iopub.status.busy": "2021-10-12T08:53:51.181938Z",
     "iopub.status.idle": "2021-10-12T08:54:11.681379Z",
     "shell.execute_reply": "2021-10-12T08:54:11.680898Z",
     "shell.execute_reply.started": "2021-10-12T08:53:51.182367Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/12 08:53:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hoge: double (nullable = true)\n",
      " |-- foo: string (nullable = true)\n",
      " |-- bar: double (nullable = true)\n",
      " |-- boo: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session(\"localstack\", conf)\n",
    "\n",
    "try:\n",
    "    df = spark.read.parquet('s3a://data/test/test.parquet/')\n",
    "    df.printSchema()\n",
    "    print(df.count())\n",
    "except Exception as exp:\n",
    "    print(exp)\n",
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab91bbb5-61c6-402f-b93e-acf82782809a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T08:57:10.030191Z",
     "iopub.status.busy": "2021-10-12T08:57:10.029925Z",
     "iopub.status.idle": "2021-10-12T08:57:10.075196Z",
     "shell.execute_reply": "2021-10-12T08:57:10.074255Z",
     "shell.execute_reply.started": "2021-10-12T08:57:10.030167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from s3fs.core import S3FileSystem\n",
    "from pyarrow import Table, parquet as pq\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "def to_df(data: List[Dict[str, Any]]) -> DataFrame:                                                                                                         \n",
    "    df = DataFrame()\n",
    "    for item in data:\n",
    "        indexes = []\n",
    "        values = []\n",
    "        for k, v in item.items():\n",
    "            indexes.append(k)\n",
    "            values.append(v)\n",
    "        df = df.append(Series(values, index=indexes), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# s3fs client\n",
    "fs = S3FileSystem(\n",
    "     anon=False,\n",
    "     key=s3_key,\n",
    "     secret=s3_secret,\n",
    "     use_ssl=False,\n",
    "     client_kwargs={'endpoint_url': s3_url})\n",
    "\n",
    "path_to_s3_object = \"s3://data/test/test.parquet\"\n",
    "\n",
    "data = [\n",
    "  {\n",
    "    \"hoge\": 1,\n",
    "    \"foo\": \"blah\",\n",
    "  },\n",
    "  {\n",
    "    \"boo\": \"test\",\n",
    "    \"bar\": 123,\n",
    "  },\n",
    "]\n",
    "df = to_df(data)\n",
    "pq.write_to_dataset(\n",
    "    Table.from_pandas(df),\n",
    "    path_to_s3_object,\n",
    "    filesystem=fs,\n",
    "    use_dictionary=True,\n",
    "    #compression=\"snappy\",\n",
    "    version=\"2.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0404cf-3471-41b7-be50-cc964456e5b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T11:14:48.759965Z",
     "iopub.status.busy": "2021-10-12T11:14:48.759569Z",
     "iopub.status.idle": "2021-10-12T11:14:48.907626Z",
     "shell.execute_reply": "2021-10-12T11:14:48.907125Z",
     "shell.execute_reply.started": "2021-10-12T11:14:48.759937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from confluent_kafka import DeserializingConsumer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer\n",
    "from confluent_kafka.serialization import StringDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c52e7-6cd3-48c1-81f5-8d2e80136c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    topic = args.topic\n",
    "\n",
    "    schema_registry_conf = {'url': args.schema_registry}\n",
    "\n",
    "    schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "    avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "    \n",
    "    string_deserializer = StringDeserializer('utf_8')\n",
    "\n",
    "    consumer_conf = {'bootstrap.servers': args.bootstrap_servers,\n",
    "                     'key.deserializer': string_deserializer,\n",
    "                     'value.deserializer': avro_deserializer,\n",
    "                     'group.id': args.group,\n",
    "                     'auto.offset.reset': \"earliest\"}\n",
    "\n",
    "    consumer = DeserializingConsumer(consumer_conf)\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
    "            msg = consumer.poll(1.0)\n",
    "            print(msg.value())\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    consumer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"AvroDeserializingProducer\")\n",
    "    parser.add_argument('-b', dest=\"bootstrap_servers\",\n",
    "                        default=\"kafka.kafka.svc.cluster.local:9092\", help=\"Bootstrap servers\")\n",
    "    parser.add_argument('-r', dest=\"schema_registry\",\n",
    "                        default=\"http://schema-registry.kafka.svc.cluster.local:8085\", help=\"Schema registry url\")\n",
    "    parser.add_argument('-t', dest=\"topic\", default=\"test\", help=\"Topic\")\n",
    "    parser.add_argument('-s', dest=\"symbol\", default=\"BTCUSDT\", help=\"Symbol\")\n",
    "    parser.add_argument('-g', dest=\"group\", default=\"test\", help=\"Consumer group\")\n",
    "    main(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118248d-b411-4ac5-9d6e-5d92725408f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic = \"test\"\n",
    "\n",
    "schema_registry_conf = {\"url\": \"http://schema-registry.kafka.svc.cluster.local:8085\"}\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "\n",
    "string_deserializer = StringDeserializer('utf_8')\n",
    "\n",
    "consumer_conf = {'bootstrap.servers': \"kafka.kafka.svc.cluster.local:9092\",\n",
    "                 'key.deserializer': string_deserializer,\n",
    "                 'value.deserializer': avro_deserializer,\n",
    "                 'group.id': \"test\",\n",
    "                 'auto.offset.reset': \"earliest\"}\n",
    "\n",
    "consumer = DeserializingConsumer(consumer_conf)\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
    "        msg = consumer.poll(1.0)\n",
    "        print(msg.value())\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "consumer.close()\n",
    "\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "from s3fs.core import S3FileSystem\n",
    "from pyarrow import Table, parquet as pq\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "def to_df(data: List[Dict[str, Any]]) -> DataFrame:                                                                                                         \n",
    "    df = DataFrame()\n",
    "    for item in data:\n",
    "        indexes = []\n",
    "        values = []\n",
    "        for k, v in item.items():\n",
    "            indexes.append(k)\n",
    "            values.append(v)\n",
    "        df = df.append(Series(values, index=indexes), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# s3fs client\n",
    "fs = S3FileSystem(\n",
    "     anon=False,\n",
    "     key=s3_key,\n",
    "     secret=s3_secret,\n",
    "     use_ssl=False,\n",
    "     client_kwargs={'endpoint_url': s3_url})\n",
    "\n",
    "path_to_s3_object = \"s3://data/test/test.parquet\"\n",
    "\n",
    "data = [\n",
    "  {\n",
    "    \"hoge\": 1,\n",
    "    \"foo\": \"blah\",\n",
    "  },\n",
    "  {\n",
    "    \"boo\": \"test\",\n",
    "    \"bar\": 123,\n",
    "  },\n",
    "]\n",
    "df = to_df(data)\n",
    "pq.write_to_dataset(\n",
    "    Table.from_pandas(df),\n",
    "    path_to_s3_object,\n",
    "    filesystem=fs,\n",
    "    use_dictionary=True,\n",
    "    #compression=\"snappy\",\n",
    "    version=\"2.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b0bfd-34e0-4eff-9a23-21dfc2d587ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic = \"test\"\n",
    "\n",
    "schema_registry_conf = {\"url\": \"http://schema-registry.kafka.svc.cluster.local:8085\"}\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "\n",
    "string_deserializer = StringDeserializer('utf_8')\n",
    "\n",
    "consumer_conf = {'bootstrap.servers': \"kafka.kafka.svc.cluster.local:9092\",\n",
    "                 'key.deserializer': string_deserializer,\n",
    "                 'value.deserializer': avro_deserializer,\n",
    "                 'group.id': \"test\",\n",
    "                 #'auto.offset.reset': \"smallest\"\n",
    "                }\n",
    "\n",
    "consumer = DeserializingConsumer(consumer_conf)\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            print(\"none\")\n",
    "            continue\n",
    "        \n",
    "        if msg is not None:\n",
    "            print(hash(str(msg.value())))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "968d4091-78a6-438f-a4c1-3234714c170d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "threads can only be started once",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1490787/904296095.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtwm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_kline_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle_socket_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"threads can only be started once\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_active_limbo_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0m_limbo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: threads can only be started once"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import json\n",
    "from binance import ThreadedWebsocketManager\n",
    "from confluent_kafka import SerializingProducer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "\n",
    "# Read schema from file\n",
    "with open('schema.txt', 'r') as f:\n",
    "    schema_str = f.read()\n",
    "\n",
    "\n",
    "topic = \"avro-test2\"\n",
    "symbol = \"BTCUSDT\"\n",
    "\n",
    "schema_registry_conf = {'url': \"http://10.105.144.163:8085\"}\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "avro_serializer = AvroSerializer(schema_registry_client, schema_str)\n",
    "\n",
    "producer_conf = {'bootstrap.servers': \"10.102.117.121:9092\",\n",
    "                    'key.serializer': StringSerializer('utf_8'),\n",
    "                    'value.serializer': avro_serializer}\n",
    "\n",
    "producer = SerializingProducer(producer_conf)\n",
    "\n",
    "twm = ThreadedWebsocketManager()\n",
    "\n",
    "def handle_socket_message(msg):\n",
    "\n",
    "    print(msg)\n",
    "    \n",
    "    producer.produce(topic=topic, key=symbol, value=msg)\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < 3:\n",
    "    \n",
    "    i = i+1\n",
    " \n",
    "    twm.start()\n",
    "        \n",
    "    client = twm.start_kline_socket(callback=handle_socket_message, symbol=symbol)\n",
    "\n",
    "    # twm.join()\n",
    "    \n",
    "    twm.stop_socket(client)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
